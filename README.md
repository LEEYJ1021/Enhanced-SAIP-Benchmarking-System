# Enhanced-SAIP-Benchmarking

**An Interdisciplinary Framework for Solving the Edge AI Trilemma: Balancing Performance, Efficiency, and Value in Small Language Model Deployment**

---

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.9%2B-green.svg)]()

## üåü Overview

This repository implements a comprehensive **Decision Support System (DSS)** that addresses the "Edge AI Trilemma" through an interdisciplinary framework integrating **Information Systems Success Theory**, **Technology Acceptance Models**, and **Production Economics**. 

Analyzing 5,400 observations across 9 models (0.5B-3.2B parameters) and 3 tasks, we demonstrate that smaller, optimized models consistently outperform larger alternatives in holistic value assessment. The framework transforms edge AI deployment from intuitive art to evidence-based science.

### üéØ Key Capabilities

* **Multi-Dimensional Evaluation**: Technical performance, economic value, and adoption risk assessment
* **Decision Support System**: Validated deployment rules (OSQ‚â•0.75, ARI‚â•0.80) with 94% classification accuracy
* **Econometric Analysis**: Production functions, technical efficiency frontiers, and cost modeling
* **Behavioral Modeling**: Operationalized TAM constructs (PU, PEOU, BI) for adoption prediction
* **Adaptive Optimization**: SAIP policy delivering 30-45% latency reduction
* **Enterprise Integration**: ROI analysis, risk-return portfolios, and implementation roadmaps

### üèÜ Key Findings

* **Qwen2.5-0.5B** achieved superior Overall System Quality (OSQ=0.782) and Adoption Readiness (ARI=0.845) while operating at 100% technical efficiency
* **Strong diminishing returns to scale** (elasticity=0.391, p<0.001) quantified the inefficiency of parameter scaling
* **Llama-3.2-3B** exhibited only 20.7% efficiency despite higher parameter count
* **SAIP adaptive policy** delivers 30-45% latency reduction for eligible models

---

## üìÅ Repository Structure

```
Enhanced-SAIP-Benchmarking/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_analysis.json
‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_results.xlsx
‚îÇ   ‚îú‚îÄ‚îÄ kpi_summary.csv
‚îÇ   ‚îî‚îÄ‚îÄ raw_results.csv
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ cost_function_summary.txt
‚îÇ   ‚îú‚îÄ‚îÄ panel_data_model_summary.txt
‚îÇ   ‚îî‚îÄ‚îÄ production_function_summary.txt
‚îÇ
‚îú‚îÄ‚îÄ visualizations/
‚îÇ   ‚îú‚îÄ‚îÄ is_quality_dashboard.png
‚îÇ   ‚îú‚îÄ‚îÄ performance_heatmap.png
‚îÇ   ‚îú‚îÄ‚îÄ policy_effectiveness.png
‚îÇ   ‚îú‚îÄ‚îÄ tam_analysis.png
‚îÇ   ‚îî‚îÄ‚îÄ technical_efficiency.png
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ sLLM_MIS_ÎÖºÎ¨∏Î∞òÏòÅ.ipynb
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.9+
- 8GB RAM minimum (16GB recommended)
- 10GB disk space for datasets and models

### Installation

```bash
git clone https://github.com/your-username/Enhanced-SAIP-Benchmarking.git
cd Enhanced-SAIP-Benchmarking
pip install -r requirements.txt
```

### Run the Main Analysis Notebook

All analyses, visualizations, and econometric modeling are orchestrated from the main Jupyter Notebook:

```bash
jupyter notebook sLLM_MIS_ÎÖºÎ¨∏Î∞òÏòÅ.ipynb
```

Run all cells sequentially to reproduce the entire pipeline. The notebook will automatically load the required datasets, run econometric and IS-theory-based analyses, and generate visualization outputs.

---

## ‚öôÔ∏è Configuration

### Model Specifications

The framework evaluates 9 sLLMs across the optimal edge deployment range:

| Model Family | Variant | Parameters | Distinctive Features |
|-------------|---------|------------|---------------------|
| Qwen2.5 | Qwen2.5-0.5B/1.5B/3B-Instruct | 0.5B, 1.5B, 3.0B | Scalable instruction-tuned series |
| EXAONE | EXAONE-3.5-2.4B-Instruct | 2.4B | Bilingual Korean-English optimization |
| Gemma | Gemma-2B-IT | 2.0B | Instruction-tuned on diverse corpora |
| DeepSeek | DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | Knowledge distillation for enhanced reasoning |
| Llama | Llama-3.2-3B-Instruct | 3.2B | Multilingual instruction tuning |
| MIDM | MIDM-2.0-Mini-Instruct | 2.0B | Optimized for Korean language and low-latency |
| OPT | OPT-2.7B | 2.7B | GPT-3 aligned architecture |

### Evaluation Protocol

- **Tasks**: NLU (SST-2), QA (SQuAD v2), Summarization (CNN/DailyMail)
- **Trials**: 3 independent runs per task-model pair
- **Samples**: 100 instances per task (N=300 observations per model-task)
- **Hardware**: Rebellions ATOM‚Ñ¢ NPU with Intel Core i9 CPU baseline
- **Prompt Strategy**: Zero-shot for unbiased comparison

---

## üß† Theoretical Framework

### 1. Information Systems Success Model (DeLone & McLean)

**Operationalized Constructs:**
- **System Quality**: Inference latency (P95), throughput, stability (CV)
- **Information Quality**: F1-Score, ROUGE-1, semantic coherence
- **Service Quality**: Energy efficiency, memory utilization, deployment overhead

**Composite Metric:**
```
OSQ = 0.40 √ó Q_system + 0.30 √ó Q_information + 0.30 √ó Q_service
```

### 2. Technology Acceptance Model (TAM)

**Operationalized Constructs:**
- **Perceived Usefulness (PU)**: Accuracy, throughput, consistency
- **Perceived Ease of Use (PEOU)**: Latency stability, energy efficiency, resource accessibility
- **Behavioral Intention (BI)**: Adoption Readiness Index (ARI)

**Adoption Prediction:**
```
ARI = 0.65 √ó PU + 0.45 √ó PEOU + 0.25 √ó (PU √ó PEOU) + 0.10 √ó e^(-Deployment_complexity)
```

### 3. Econometric Production Analysis

**Cobb-Douglas Production Function:**
```
ln(Output) = ln(A) + Œ± √ó ln(Parameters) + Œ≤ √ó ln(Power) + Œµ
```

**Technical Efficiency:**
- Data Envelopment Analysis (DEA) for efficiency frontier
- Relative efficiency scores (0.207-1.000) across model configurations

---

## üìä Decision Support System

### Validated Deployment Rules

| Rule | Condition | Action | Empirical Support |
|------|-----------|--------|-------------------|
| **R1** | OSQ ‚â• 0.75 AND ARI ‚â• 0.80 | Immediate deployment on edge NPU | Qwen2.5-0.5B (OSQ: 0.782, ARI: 0.845) |
| **R2** | 0.60 ‚â§ OSQ < 0.75 OR P95 ‚â• 1200ms | Apply SAIP policy and deploy if ‚â•25% improvement | EXAONE-2.4B showed 44.1% latency reduction |
| **R3** | Parameters ‚â• 2.5B AND P95 ‚â• 2000ms | Defer to cloud or implement compression | Llama-3.2-3B (0.207 efficiency) breached both thresholds |
| **R4** | OSQ < 0.60 OR ARI < 0.65 | Require pilot testing and optimization | Applied to lower-performing configurations |

### Economic Decision Framework

**ROI Calculation:**
```
ROI_30days = (Net_Revenue_30 - Total_Cost) / Total_Cost √ó 100
```

**Risk Assessment:**
```
Deployment_Risk = 0.35 √ó CV_latency + 0.25 √ó Size_complexity + 
                  0.20 √ó (1 - ARI) + 0.20 √ó Integration_difficulty
```

---

## üìà Key Results & Visualizations

### Hypothesis Validation Summary

| Hypothesis | Research Area | Key Finding | Status | Evidence |
|------------|---------------|-------------|---------|----------|
| **H1** | IS Success Model | Technical performance ‚Üí Higher IS Quality | ‚úÖ Supported | Pearson r=0.83, p<0.001 |
| **H2** | TAM | Performance + Efficiency ‚Üí Higher Adoption | ‚úÖ Supported | R¬≤=0.72, p<0.001 |
| **H3** | Econometrics | Diminishing returns to scale confirmed | ‚úÖ Supported | Elasticity=0.391, p<0.001 |
| **H4** | Adaptive Policy | SAIP reduces latency (30-45% gains) | ‚úÖ Supported | 44.1% reduction for EXAONE-2.4B |

### Model Performance Ranking

1. **Qwen2.5-0.5B-Instruct**: Best overall (OSQ=0.782, ARI=0.845, DEA=1.000)
2. **EXAONE-3.5-2.4B-Instruct**: Strong balance (OSQ=0.725, ARI=0.756)
3. **Gemma-2B-IT**: Competitive mid-range (OSQ=0.698, ARI=0.723)
4. **DeepSeek-1.5B**: Efficient distillation (OSQ=0.687, ARI=0.701)
5. **MIDM-2.0-Mini**: Language specialization (OSQ=0.675, ARI=0.689)

### Economic Value Assessment

| Model | 30-Day ROI | Deployment Cost | Risk Category | Recommendation |
|-------|------------|----------------|---------------|----------------|
| Qwen2.5-0.5B | 85% | $2.1K | Low | Immediate Deployment |
| EXAONE-2.4B | 45% | $8.7K | Medium | Deploy with SAIP |
| Gemma-2B | 35% | $6.2K | Medium | Deploy with SAIP |
| Llama-3.2B | -15% | $15.3K | High | Defer to Cloud |

---

## üé® Output Artifacts

### Decision Support Visualizations

1. **DSS Scorecard**: Integrated multi-metric assessment heatmap
2. **Decision Quadrant**: OSQ vs ARI deployment matrix with threshold boundaries  
3. **ROI Analysis**: Economic value creation vs performance trade-offs
4. **Risk-Return Matrix**: Portfolio-style investment decision framework
5. **Efficiency Frontier**: Technical efficiency boundary analysis
6. **SAIP Decision Tree**: Adaptive policy application guidelines
7. **Implementation Timeline**: Phased risk-managed rollout strategy
8. **Sensitivity Analysis**: Parameter impact on decision outcomes
9. **Executive Matrix**: Strategic overview by deployment strategy

### Data Outputs

- `comprehensive_analysis.json`: Complete analysis results with confidence intervals
- `kpi_summary.csv`: Aggregated performance indicators across all dimensions

---

## üî¨ Methodology

### Statistical Validation

- **Bootstrap Confidence Intervals**: B=1,000 samples for all composite indices
- **Cross-Validation**: Stratified 5-fold partitioning by model family
- **Sensitivity Analysis**: ¬±20% parameter perturbation with Latin Hypercube Sampling
- **Robustness Protocols**: Outlier detection, temporal stability, multi-model validation

### Econometric Specifications

| Model Type | Functional Form | Estimation Method | R¬≤ | Key Finding |
|------------|----------------|-------------------|----|-------------|
| Production Function | Cobb-Douglas log-linear | OLS with robust SE | 0.283 | Diminishing returns (elasticity=0.391) |
| Cost Function | Log-linear | OLS with robust SE | 0.307 | Cost elasticity = -0.653 |
| Technical Efficiency | DEA-based ratio | Linear programming | 0.842 | Mean efficiency = 0.842 |
| Panel Data Model | Fixed effects linear | Within-group OLS | 0.491 | Task effects significant (p<0.001) |

---

## üõ†Ô∏è Advanced Usage

### Custom Model Integration

```python
from src.decision_support import DecisionSupportSystem
from config.model_config import ModelConfig

# Add custom model specification
custom_model = ModelConfig(
    name="Custom-Model-1B",
    parameters=1.2,
    latency_p95=850,
    throughput=45.2,
    accuracy=0.78,
    energy_consumption=0.0032
)

# Evaluate deployment readiness
dss = DecisionSupportSystem()
recommendation = dss.evaluate_model(custom_model)
print(f"Deployment Recommendation: {recommendation}")
```

### SAIP Policy Customization

```python
from src.saip_policy import SAIPPolicyEngine

# Custom policy thresholds
policy_engine = SAIPPolicyEngine(
    latency_threshold=1500,  # ms
    variability_threshold=0.15,  # CV
    expected_improvement=0.25  # 25% minimum
)

# Apply to model portfolio
optimized_models = policy_engine.optimize_portfolio(model_data)
```

---

## üìú License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

* **Rebellions Inc.** for NPU access and infrastructure support
* **Open-source AI community** for model architectures and benchmarking tools
* **Research collaborators** for theoretical guidance and validation
* **Beta testers** for framework refinement and real-world testing

---

<!-- EOF -->
